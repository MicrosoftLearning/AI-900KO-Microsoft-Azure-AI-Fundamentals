{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 얼굴 감지 및 분석\r\n",
        "\r\n",
        "Computer Vision 솔루션은 종종 사람의 얼굴을 감지, 분석, 식별할 수 있는 AI(인공 지능) 솔루션을 필요로 합니다. 예를 들어 소매업체 Northwind Traders가 AI 서비스를 통해 매장을 모니터링하여 도움이 필요한 고객을 파악하고 직원을 보내 돕도록 하는 \"스마트 스토어\"를 구현하기로 결정했다고 가정해 보겠습니다. 이를 달성하기 위한 한 가지 방법은 얼굴 감지 및 분석을 수행하는 것입니다. 즉, 이미지에 얼굴이 있는지 확인하고, 얼굴이 있다면 그 특징을 분석하는 것입니다.\r\n",
        "\r\n",
        "![얼굴을 분석하는 로봇](./images/face_analysis.jpg)\r\n",
        "\r\n",
        "## Face Cognitive Service를 사용하여 얼굴 감지\r\n",
        "\r\n",
        "Northwind Traders가 만들고자 하는 스마트 스토어 시스템이 고객을 감지하고 고객의 얼굴 특징을 분석할 수 있어야 한다고 가정해 보세요. Microsoft Azure에서는 Azure Cognitive Services에 포함된 **Face**를 사용하여 이 작업을 수행할 수 있습니다.\r\n",
        "\r\n",
        "### Cognitive Services 리소스 만들기\r\n",
        "\r\n",
        "먼저 Azure 구독에서 **Cognitive Services** 리소스를 만들어 보겠습니다.\r\n",
        "\r\n",
        "> **참고**: 이미 Cognitive Services 리소스를 보유하고 있다면 Azure Portal에서 **빠른 시작** 페이지를 열고 키 및 엔드포인트를 아래의 셀로 복사하기만 하면 됩니다. 리소스가 없다면 아래의 단계를 따라 리소스를 만듭니다.\r\n",
        "\r\n",
        "1. 다른 브라우저 탭에서 Azure Portal(https://portal.azure.com) 을 열고 Microsoft 계정으로 로그인합니다.\r\n",
        "2. **&#65291;리소스 만들기** 단추를 클릭하고, *Cognitive Services*를 검색하고, 다음 설정을 사용하여 **Cognitive Services** 리소스를 만듭니다.\r\n",
        "    - **구독**: *사용자의 Azure 구독*.\r\n",
        "    - **리소스 그룹**: *고유한 이름의 새 리소스 그룹을 선택하거나 만듭니다*.\r\n",
        "    - **지역**: *사용 가능한 지역을 선택합니다*.\r\n",
        "    - **이름**: *고유한 이름을 입력합니다*.\r\n",
        "    - **가격 책정 계층**: S0\r\n",
        "    - **알림을 읽고 이해했음을 확인합니다**. 선택됨.\r\n",
        "3. 배포가 완료될 때까지 기다립니다. 그런 다음에 Cognitive Services 리소스로 이동하고, **개요** 페이지에서 링크를 클릭하여 서비스의 키를 관리합니다. 클라이언트 애플리케이션에서 Cognitive Services 리소스에 연결하려면 엔드포인트 및 키가 필요합니다.\r\n",
        "\r\n",
        "### Cognitive Services 리소스의 키 및 엔드포인트 가져오기\r\n",
        "\r\n",
        "Cognitive Services 리소스를 사용하려면 클라이언트 애플리케이션에 해당 엔드포인트 및 인증 키가 필요합니다.\r\n",
        "\r\n",
        "1. Azure Portal에 있는 Cognitive Service 리소스의 **키 및 엔드포인트** 페이지에서 리소스의 **Key1**을 복사하고 아래 코드에 붙여 넣어 **YOUR_COG_KEY**를 대체합니다.\r\n",
        "\r\n",
        "2. 리소스의 **엔드포인트**를 복사하고 아래 코드에 붙여 넣어 **YOUR_COG_ENDPOINT**를 대체합니다.\r\n",
        "\r\n",
        "3. 셀 실행 <span>&#9655;</span> 단추(셀 왼쪽 상단에 있음)를 클릭하여 아래의 셀에 있는 코드를 실행합니다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693964655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 Face 서비스를 사용하여 매장에서 사람 얼굴을 감지할 수 있는 Cognitive Services 리소스가 생겼습니다.\r\n",
        "\r\n",
        "아래의 코드 셀을 실행하여 예시를 확인해 보세요."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import faces\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# 얼굴 감지 클라이언트를 만듭니다.\r\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# 이미지 열기\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# 얼굴 감지\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# 얼굴 표시(python_code/faces.py에 있는 코드)\r\n",
        "faces.show_faces(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970079
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "감지된 얼굴 각각에 고유한 ID가 할당되므로 애플리케이션은 감지된 각각의 얼굴을 식별할 수 있습니다.\r\n",
        "\r\n",
        "아래의 셀을 실행하여 좀 더 많은 쇼핑객 얼굴의 ID를 표시해 보세요."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 열기\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# 얼굴 감지\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# 얼굴 표시(python_code/faces.py에 있는 코드)\r\n",
        "faces.show_faces(image_path, detected_faces, show_id=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 얼굴 특성 분석\r\n",
        "\r\n",
        "Face는 얼굴을 감지하는 것 외에도 많은 일들을 할 수 있습니다. 얼굴 특징 및 표현을 분석하여 연령과 감정 상태를 파악할 수도 있습니다. 예를 들어 아래 코드를 실행하여 쇼핑객의 얼굴 특성을 분석해 보세요."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 열기\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# 얼굴 및 지정된 얼굴 특성 감지\r\n",
        "attributes = ['age', 'emotion']\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
        "\n",
        "# 얼굴 및 특성 표시(python_code/faces.py에 있는 코드)\r\n",
        "faces.show_face_attributes(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693971321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지에서 고객에 대해 감지된 감정 점수를 바탕으로 판단할 때, 이 고객은 쇼핑 경험에 꽤 만족한 것처럼 보입니다.\r\n",
        "\r\n",
        "## 유사 얼굴 찾기 \r\n",
        "\r\n",
        "감지된 각 얼굴에 대해 생성된 얼굴 ID는 얼굴 감지를 개별적으로 식별하는 데 사용됩니다. 이 ID를 사용하여 감지된 얼굴을 이전에 감지된 얼굴과 비교하고 유사한 특징의 얼굴을 찾을 수 있습니다.\r\n",
        "\r\n",
        "예를 들어 아래의 셀을 실행하여 한 이미지에 있는 쇼핑객과 다른 이미지에 있는 쇼핑객을 비교하여 일치하는 얼굴을 찾아보세요."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 1에서 첫 번째 얼굴의 ID 가져오기\r\n",
        "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_1_stream = open(image_1_path, \"rb\")\n",
        "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
        "face_1 = image_1_faces[0]\n",
        "\n",
        "# 두 번째 이미지에서 얼굴 ID 가져오기\r\n",
        "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_2_stream = open(image_2_path, \"rb\")\n",
        "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
        "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
        "\n",
        "# 이미지 1에 있는 얼굴과 비슷한 얼굴을 이미지 2에서 찾기\r\n",
        "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
        "\n",
        "# 이미지 1에서 얼굴을 표시하고 이미지 2에서 비슷한 얼굴 표시(python_code/face.py에 있는 코드)\r\n",
        "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693972555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 얼굴 인식\r\n",
        "\r\n",
        "지금까지, Face가 얼굴과 얼굴 특징을 감지할 수 있고 서로 유사한 두 얼굴을 식별할 수 있다는 것을 확인했습니다. 이제 특정인의 얼굴을 인식하도록 Face를 학습시킬 수 있는 *얼굴 인식* 솔루션을 구현함으로써 한 단계 더 나아갈 수 있습니다. 이것은 소셜 미디어 애플리케이션에서 친구의 사진을 자동으로 태깅하거나 얼굴 인식을 생체 인식 확인 시스템의 일부분으로 사용하는 등 다양한 시나리오에서 유용할 수 있습니다.\r\n",
        "\r\n",
        "이것이 어떻게 작동하는지 살펴보기 위해 Northwind Traders 회사가 얼굴 인식을 사용하여 IT 부서의 인증된 직원에게만 보안 시스템 액세스를 허용하려고 한다고 가정해 보겠습니다.\r\n",
        "\r\n",
        "먼저, 인증된 직원을 나타내는 *사람 그룹*을 만들겠습니다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "group_id = 'employee_group_id'\n",
        "try:\n",
        "    # 이미 존재하는 경우 그룹 삭제\n",
        "    face_client.person_group.delete(group_id)\n",
        "except Exception as ex:\n",
        "    print(ex.message)\n",
        "finally:\n",
        "    face_client.person_group.create(group_id, 'employees')\n",
        "    print ('Group created!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693973492
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*사람 그룹*이 있으면 그룹에 포함하려는 각 직원에 대해 *사람*을 추가한 후에 각각의 사람에 대한 여러 사진을 등록하여 Face가 각자의 얼굴 특징을 학습하도록 할 수 있습니다. 같은 사람을 여러 가지 포즈와 얼굴 표현으로 보여주는 이미지를 사용하는 것이 좋습니다.\r\n",
        "\r\n",
        "Wendell이라는 직원을 추가하고 이 직원의 사진을 세 장 등록하겠습니다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# 그룹에 사람(Wendell) 추가\r\n",
        "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
        "\n",
        "# Wendell의 사진 가져오기\r\n",
        "folder = os.path.join('data', 'face', 'wendell')\n",
        "wendell_pics = os.listdir(folder)\n",
        "\n",
        "# 사진 등록\r\n",
        "i = 0\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "for pic in wendell_pics:\n",
        "    # 사람 그룹의 개개인에 각각 사진 추가\n",
        "    img_path = os.path.join(folder, pic)\n",
        "    img_stream = open(img_path, \"rb\")\n",
        "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
        "\n",
        "    # 각 이미지 표시\n",
        "    img = Image.open(img_path)\n",
        "    i +=1\n",
        "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693976898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람을 추가하고 사진을 등록했으면 이제 Face가 각 사람을 인식하도록 학습시킬 수 있습니다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "face_client.person_group.train(group_id)\n",
        "print('Trained!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693977046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 모델 학습이 완료되었으므로 이 모델을 사용하여 이미지에서 인식된 얼굴을 식별할 수 있습니다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 번째 이미지에서 얼굴 ID 가져오기\r\n",
        "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
        "\n",
        "# 인식된 얼굴 이름 가져오기\r\n",
        "face_names = {}\n",
        "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
        "for face in recognized_faces:\n",
        "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
        "    face_names[face.face_id] = person_name\n",
        "\n",
        "# 인식된 얼굴 표시\r\n",
        "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693994820
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자세한 내용\r\n",
        "\r\n",
        "Face Cognitive Service에 대해 자세히 알아보려면 [Face 설명서](https://docs.microsoft.com/azure/cognitive-services/face/)를 참조하세요.\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}